# ==============================================================================
# CM-DLSSM Artifact: Main Configuration File
# Path: configs/config.yaml
# ==============================================================================
# Description:
#   This is the central configuration file for all CM-DLSSM experiments.
#   It uses Hydra's composition system to manage complex experiment setups.
#
# Usage:
#   python experiments/01_benchmark_main.py
#   python experiments/02_long_context_evidence.py
#   python experiments/03_ablation_matrix.py
#   python experiments/03_ablation_matrix.py ablation.variant=B
#   python experiments/04_audit_reproducibility.py
# ==============================================================================

defaults:
  - _self_
  - override hydra/hydra_logging: default
  - override hydra/job_logging: default

# ==============================================================================
# General Settings
# ==============================================================================
seed: 42
debug: false
simulation_mode: true  # Set to false for real model evaluation

# Device configuration
device: "cuda"  # Options: "cuda", "cpu", "mps" (for Apple Silicon)
use_amp: false  # Automatic Mixed Precision (requires CUDA)

# ==============================================================================
# Model Architecture Settings
# ==============================================================================
model:
  # Core SSM (State Space Model) parameters
  d_model: 1024          # Hidden dimension (Paper: 1024)
  d_state: 128           # SSM state dimension (Paper: 128)
  d_conv: 4              # Convolution kernel size
  expand: 2              # Expansion factor for MLP
  
  # Architecture depth
  num_layers: 4          # Number of Mamba blocks (Paper: 4-6)
  
  # Regularization
  dropout: 0.1
  layer_norm_eps: 1e-5
  
  # SRVS Gate settings
  num_sink_types: 128    # Size of vulnerability sink taxonomy
  gate_dim: 64           # Gating projection dimension
  
  # Logic Layer settings
  num_logic_rules: 256   # Differentiable logic rules
  logic_temp: 0.5        # Temperature for soft logic

# ==============================================================================
# Training Settings
# ==============================================================================
training:
  # Optimization
  batch_size: 16
  learning_rate: 0.0001
  weight_decay: 0.01
  num_epochs: 10
  warmup_steps: 1000
  
  # Gradient management
  max_grad_norm: 1.0
  gradient_accumulation_steps: 4
  
  # Learning rate schedule
  scheduler: "cosine"  # Options: "cosine", "linear", "constant"
  min_lr: 1e-6
  
  # Early stopping
  patience: 3
  min_delta: 0.001

# ==============================================================================
# Evaluation Settings
# ==============================================================================
evaluation:
  batch_size: 32
  num_samples: 1000      # Number of samples for quick evaluation
  
  # Metrics to compute
  compute_per_cwe: true  # Per-CWE breakdown (Table 2)
  compute_pr_curve: true # Precision-Recall curve
  
  # Security-specific thresholds
  target_fpr: 0.01       # Target False Positive Rate (1%)
  confidence_threshold: 0.5

# ==============================================================================
# Long Context Experiment Settings (Experiment 02)
# ==============================================================================
long_context:
  # Bucket boundaries (in tokens)
  buckets:
    - [0, 1024, "Short (0-1k)"]
    - [1024, 4096, "Medium (1k-4k)"]
    - [4096, 16384, "Long (4k-16k)"]
    - [16384, 65536, "Very Long (16k-64k)"]
    - [65536, 131072, "Extreme (64k+)"]
  
  # Degradation curve test points
  test_lengths: [2048, 8192, 16384, 32768, 65536, 128000]
  
  # Baseline models to compare
  baselines:
    - "CodeBERT-512"
    - "RAG-16k"
    - "CM-DLSSM"

# ==============================================================================
# Ablation Study Configuration (Experiment 03)
# ==============================================================================
ablation:
  # Current variant to test (A, B, C, or D)
  variant: "D"
  
  # Variant definitions
  variants:
    A:
      description: "Baseline (Mamba only - Raw Neural Sensing)"
      modules:
        logic_layer: false
        calibration_vault: false
        compliance_gate: false
    
    B:
      description: "Baseline + Logic Layer (CAVI Inference)"
      modules:
        logic_layer: true
        calibration_vault: false
        compliance_gate: false
    
    C:
      description: "Baseline + Logic + Calibration (Two-Stage Vault)"
      modules:
        logic_layer: true
        calibration_vault: true
        compliance_gate: false
    
    D:
      description: "Full System (+ Compliance Gate with Abstain)"
      modules:
        logic_layer: true
        calibration_vault: true
        compliance_gate: true
      gate_thresholds:
        eta_09: 0.0001        # Flip threshold (Section 5.2)
        min_attachment: 0.98  # Coverage threshold
  
  # Shared evaluation settings for ablation
  shared:
    batch_size: 32
    num_samples: 5000
    eval_mode: "strict"  # Strict evaluation for ablation

# ==============================================================================
# Audit Reproducibility Settings (Experiment 04)
# ==============================================================================
n_samples: 1000          # Top-level for backward compatibility
epsilon: 0.00001         # Top-level for backward compatibility (1e-5)

audit_reproducibility:
  # Number of VAAs to generate and verify
  n_samples: 1000
  
  # Tolerance for reproducibility check
  epsilon: 0.00001       # 1e-5 tolerance for cross-platform consistency
  
  # CAVI engine parameters
  num_predicates: 3      # Simplified for batch testing (a, b, c)
  max_iterations: 5      # Number of CAVI iterations
  damping: 0.5           # Damping factor for message passing
  
  # Verification thresholds
  thresholds:
    pass_rate_critical: 95.0    # Below this is critical failure
    pass_rate_warning: 99.0     # Below this is warning
    pass_rate_target: 99.9      # Target pass rate (paper: 99.2%)
  
  # Hardware simulation
  prover_device: "auto"  # "auto", "cuda", "cpu"
  prover_dtype: "float32"
  verifier_dtype: "float64"
  
  # Output settings
  save_failed_vaas: true
  generate_report: true
  verbose: false

# ==============================================================================
# Causal Inference Settings (Experiment 05)
# ==============================================================================
causal:
  # Treatment variable
  treatment: "has_auth_check"  # Binary: presence of authentication
  
  # Outcome variable
  outcome: "vuln_severity"     # Continuous: vulnerability severity score
  
  # Confounders to adjust for
  confounders:
    - "code_complexity"
    - "developer_experience"
    - "project_maturity"
  
  # Estimation methods
  methods:
    - "ipw"           # Inverse Propensity Weighting
    - "dr"            # Doubly Robust
    - "matching"      # Propensity Score Matching
  
  # Bootstrap settings for confidence intervals
  num_bootstrap: 1000
  confidence_level: 0.95
  
  # Sensitivity analysis
  sensitivity:
    enabled: true
    gamma_range: [1.0, 1.5, 2.0, 2.5, 3.0]  # Rosenbaum bounds

# ==============================================================================
# Data Paths
# ==============================================================================
paths:
  # Input data
  data_dir: "data/processed"
  raw_data_dir: "data/raw"
  
  # Model checkpoints
  checkpoint_dir: "checkpoints"
  best_model: "checkpoints/best_model.pt"
  
  # Outputs
  output_dir: "artifacts/results"
  figure_dir: "artifacts/figures"
  vaa_dir: "artifacts/vaa"
  
  # Logs
  log_dir: "logs"
  tensorboard_dir: "runs"

# ==============================================================================
# Dataset Settings
# ==============================================================================
dataset:
  # BigVul dataset
  bigvul:
    train_split: 0.8
    val_split: 0.1
    test_split: 0.1
    max_samples: null  # null = use all data
  
  # Synthetic BLV dataset
  blv:
    train_size: 50000
    test_size: 10000
    confounding_strength: 0.7
  
  # Tokenization
  max_seq_length: 131072  # Maximum context length (128k tokens)
  tokenizer: "microsoft/codebert-base"
  
  # Data augmentation
  augmentation:
    enabled: false
    prob_insert_noise: 0.1
    prob_shuffle_lines: 0.05

# ==============================================================================
# Logging and Monitoring
# ==============================================================================
logging:
  level: "INFO"  # Options: DEBUG, INFO, WARNING, ERROR
  log_to_file: true
  log_to_console: true
  
  # Weights & Biases integration
  wandb:
    enabled: false
    project: "cm-dlssm"
    entity: null
    tags: ["artifact", "reproduction"]
  
  # TensorBoard
  tensorboard:
    enabled: false
  
  # Checkpointing
  save_every_n_epochs: 1
  keep_last_n_checkpoints: 3

# ==============================================================================
# Reproducibility
# ==============================================================================
reproducibility:
  deterministic: true
  benchmark: false  # Set to true for performance, false for reproducibility

# ==============================================================================
# VAA (Verifiable Audit Artifact) Generation
# ==============================================================================
vaa:
  enabled: true
  
  # What to include in the audit trail
  include:
    model_architecture: true
    training_history: true
    evaluation_metrics: true
    gating_statistics: true
    logic_rule_activations: true
    causal_estimates: true
  
  # Cryptographic signing
  sign_artifacts: false
  signing_key: null
  
  # Output format
  format: "json"  # Options: json, yaml, pdf
  
  # Compliance standards
  standards:
    - "ISO27034"  # Application Security
    - "NIST-SSDF" # Secure Software Development Framework

# ==============================================================================
# Hydra Configuration
# ==============================================================================
hydra:
  run:
    dir: outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  
  sweep:
    dir: multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}
  
  job:
    name: cm_dlssm_experiment
    chdir: false

# ==============================================================================
# Experiment-Specific Presets
# ==============================================================================

# Preset for quick testing (override with: +preset=quick)
quick_test:
  simulation_mode: true
  evaluation:
    num_samples: 100
  ablation:
    shared:
      num_samples: 500
  audit_reproducibility:
    n_samples: 100
  causal:
    num_bootstrap: 100

# Preset for full evaluation (override with: +preset=full)
full_eval:
  simulation_mode: false
  evaluation:
    num_samples: null  # Use all data
  ablation:
    shared:
      num_samples: 10000
  audit_reproducibility:
    n_samples: 5000
  causal:
    num_bootstrap: 5000

# Preset for paper reproduction (override with: +preset=paper)
paper_reproduction:
  simulation_mode: false
  model:
    d_model: 1024
    d_state: 128
    num_layers: 6
  training:
    batch_size: 16
    num_epochs: 50
  evaluation:
    compute_per_cwe: true
    compute_pr_curve: true
  audit_reproducibility:
    n_samples: 10000
    epsilon: 0.00001

# ==============================================================================
# Advanced Settings (For Developers)
# ==============================================================================
advanced:
  # Profiling
  profile: false
  profile_memory: false
  
  # Debugging
  detect_anomaly: false  # PyTorch autograd anomaly detection
  print_model_summary: false
  
  # Optimization
  compile_model: false  # PyTorch 2.0 compile (experimental)
  use_flash_attention: false  # Requires flash-attn package
  
  # Distributed training
  distributed:
    enabled: false
    backend: "nccl"
    world_size: 1
    rank: 0
  
  # Memory optimization
  gradient_checkpointing: false
  mixed_precision: false

# ==============================================================================
# Experiment Metadata (for VAA)
# ==============================================================================
metadata:
  paper_title: "CM-DLSSM: A Cross-Modal Differentiable Logic State Space Model"
  paper_venue: "IEEE S&P 2026"
  artifact_version: "1.0.0"
  reproduction_date: null  # Will be set automatically
  evaluator: null  # Set by user
  
  # Expected results (from paper)
  expected_results:
    table2_f1: 0.924
    table2_fpr: 0.021
    table2_recall_at_1fpr: 0.89
    figure7_128k_f1: 0.91
    table5_variant_d_fpr: 0.009
    figure8_ate: -3.26
    exp04_pass_rate: 99.2  # Audit reproducibility pass rate

# ==============================================================================
# Security and Compliance Settings
# ==============================================================================
security:
  # CWE categories to focus on
  priority_cwes:
    - "CWE-79"   # XSS
    - "CWE-89"   # SQL Injection
    - "CWE-787"  # Out-of-bounds Write
    - "CWE-416"  # Use After Free
    - "CWE-20"   # Improper Input Validation
  
  # Risk thresholds
  risk_thresholds:
    critical: 0.9
    high: 0.7
    medium: 0.5
    low: 0.3
  
  # Audit requirements
  audit:
    require_explanation: true
    min_confidence: 0.8
    max_abstain_rate: 0.15

# ==============================================================================
# Performance Benchmarks (for validation)
# ==============================================================================
benchmarks:
  # Expected inference times (in seconds)
  inference_time:
    short_context_512: 0.05
    medium_context_4k: 0.2
    long_context_16k: 0.8
    extreme_context_128k: 6.0
  
  # Memory usage (in GB)
  memory_usage:
    model_params: 1.2
    peak_inference_128k: 8.0
    training_batch_16: 16.0
  
  # Throughput (samples per second)
  throughput:
    batch_1: 20
    batch_16: 200
    batch_32: 350
  
  # Audit verification throughput
  audit_verification:
    vaas_per_second: 500
    max_verification_time: 0.01  # seconds per VAA
